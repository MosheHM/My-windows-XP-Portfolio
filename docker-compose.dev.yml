# Docker Compose Development Environment
# 
# NOTE: Kubernetes is the recommended deployment method for production.
# This Docker Compose configuration is primarily for local development and testing.
# For production deployments, see k8s/ directory and run: ./scripts/deploy-k8s.sh
#
# Usage: docker-compose -f docker-compose.dev.yml up

services:
  # LLM Service (Development)
  llm-service:
    build:
      context: ./services/llm-service
      dockerfile: Dockerfile
    container_name: llm-service-dev
    environment:
      - LLM_MODEL_NAME=TinyLlama/TinyLlama-1.1B-Chat-v1.0
      - LLM_MAX_LENGTH=2048
      - LLM_TEMPERATURE=0.7
      - RAG_DATA_FILE=/app/data/resume_data.json
    volumes:
      - llm-model-cache:/root/.cache
      - ./services/llm-service/data:/app/data
      # Mount source code for hot reload in development
      - ./services/llm-service:/app
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import requests; requests.get('http://localhost:8000/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

  # File Service (Development)
  file-service:
    build:
      context: ./services/file-service
      dockerfile: Dockerfile
    container_name: file-service-dev
    environment:
      - STORAGE_PATH=/data/files
      - METADATA_PATH=/data/metadata
      - MAX_FILE_SIZE=104857600
    volumes:
      - file-storage:/data
      # Mount source code for hot reload in development
      - ./services/file-service:/app
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import requests; requests.get('http://localhost:8001/health')"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Client (Development)
  client:
    build:
      context: ./client
      dockerfile: Dockerfile
      args:
        - VITE_LLM_SERVICE_URL=/api/llm
        - VITE_FILE_SERVICE_URL=/api/files
    container_name: client-dev
    restart: unless-stopped
    depends_on:
      - llm-service
      - file-service

  # Nginx Gateway
  nginx-gateway:
    build:
      context: ./nginx
      dockerfile: Dockerfile
    container_name: nginx-gateway-dev
    ports:
      - "80:80"
    depends_on:
      - client
      - llm-service
      - file-service
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 3s
      retries: 3

volumes:
  llm-model-cache:
    driver: local
  file-storage:
    driver: local
