version: '3.8'

services:
  # LLM Service
  llm-service:
    build:
      context: ./services/llm-service
      dockerfile: Dockerfile
    container_name: llm-service
    ports:
      - "8000:8000"
    environment:
      - LLM_MODEL_NAME=TinyLlama/TinyLlama-1.1B-Chat-v1.0
      - LLM_MAX_LENGTH=2048
      - LLM_TEMPERATURE=0.7
      - RAG_DATA_FILE=/app/data/resume_data.json
    volumes:
      - llm-model-cache:/root/.cache
      - ./services/llm-service/data:/app/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import requests; requests.get('http://localhost:8000/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

  # File Service
  file-service:
    build:
      context: ./services/file-service
      dockerfile: Dockerfile
    container_name: file-service
    ports:
      - "8001:8001"
    environment:
      - STORAGE_PATH=/data/files
      - METADATA_PATH=/data/metadata
      - MAX_FILE_SIZE=104857600
    volumes:
      - file-storage:/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import requests; requests.get('http://localhost:8001/health')"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Client (Development)
  client-dev:
    build:
      context: ./client
      dockerfile: Dockerfile
    container_name: client-dev
    ports:
      - "80:80"
    environment:
      - VITE_LLM_SERVICE_URL=http://localhost:8000
      - VITE_FILE_SERVICE_URL=http://localhost:8001
    depends_on:
      - llm-service
      - file-service
    restart: unless-stopped

volumes:
  llm-model-cache:
    driver: local
  file-storage:
    driver: local
