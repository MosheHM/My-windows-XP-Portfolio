version: '3.8'

services:
  # LLM Service (Production)
  llm-service:
    build:
      context: ./services/llm-service
      dockerfile: Dockerfile
    container_name: llm-service-prod
    environment:
      - LLM_MODEL_NAME=TinyLlama/TinyLlama-1.1B-Chat-v1.0
      - LLM_MAX_LENGTH=2048
      - LLM_TEMPERATURE=0.7
      - RAG_DATA_FILE=/app/data/resume_data.json
    volumes:
      - llm-model-cache:/root/.cache
      - llm-data:/app/data
    restart: always
    healthcheck:
      test: ["CMD", "python", "-c", "import requests; requests.get('http://localhost:8000/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    # Production resource limits
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G
        reservations:
          cpus: '2'
          memory: 4G

  # File Service (Production)
  file-service:
    build:
      context: ./services/file-service
      dockerfile: Dockerfile
    container_name: file-service-prod
    environment:
      - STORAGE_PATH=/data/files
      - METADATA_PATH=/data/metadata
      - MAX_FILE_SIZE=104857600
    volumes:
      - file-storage:/data
    restart: always
    healthcheck:
      test: ["CMD", "python", "-c", "import requests; requests.get('http://localhost:8001/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
    # Production resource limits
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M

  # Client (Production)
  client:
    build:
      context: ./client
      dockerfile: Dockerfile
      args:
        - VITE_LLM_SERVICE_URL=/api/llm
        - VITE_FILE_SERVICE_URL=/api/files
    container_name: client-prod
    restart: always
    depends_on:
      - llm-service
      - file-service
    # Production resource limits
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.25'
          memory: 128M

  # Nginx Gateway (Production)
  nginx-gateway:
    build:
      context: ./nginx
      dockerfile: Dockerfile
    container_name: nginx-gateway-prod
    ports:
      - "80:80"
    depends_on:
      - client
      - llm-service
      - file-service
    restart: always
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 3s
      retries: 3
    # Production resource limits
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.25'
          memory: 128M

volumes:
  llm-model-cache:
    driver: local
  llm-data:
    driver: local
  file-storage:
    driver: local
